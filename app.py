"""
å°çº¢ä¹¦KOLå®¡ç¨¿Agent - ç½‘é¡µç‰ˆ
åŸºäº Streamlit æ„å»º
"""
import streamlit as st
import re
from datetime import datetime
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from enum import Enum
import yaml


# ============================================
# å®¡æ ¸å¼•æ“ä»£ç ï¼ˆå†…åµŒï¼‰
# ============================================

class Severity(Enum):
    """é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
    MUST_FIX = "å¿…æ”¹"
    SUGGEST = "å»ºè®®"


@dataclass
class ReviewIssue:
    """å®¡æ ¸é—®é¢˜"""
    category: str
    severity: Severity
    location: str
    original_text: str
    problem: str
    suggestion: str


@dataclass
class ReviewResult:
    """å®¡æ ¸ç»“æœ"""
    project_name: str
    kol_name: str
    version: str
    reviewer: str
    
    must_fix_issues: List[ReviewIssue] = field(default_factory=list)
    suggest_issues: List[ReviewIssue] = field(default_factory=list)
    good_points: List[str] = field(default_factory=list)
    scores: Dict[str, float] = field(default_factory=dict)
    total_score: float = 0.0


# å®¡æ ¸è§„åˆ™ï¼ˆå†…åµŒé…ç½®ï¼‰
REVIEW_RULES = {
    "project_info": {
        "name": "èƒ½æ©å…¨æŠ¤å°çº¢ä¹¦è¾¾äººç§è‰",
        "brand": "èƒ½æ©å…¨æŠ¤"
    },
    "required_keywords": {
        "æ ‡é¢˜": ["é€‚åº¦æ°´è§£", "é˜²æ•", "ç§‘æ™®"],
        "æ­£æ–‡": ["é€‚åº¦æ°´è§£", "é˜²æ•", "èƒ½æ©å…¨æŠ¤"],
        "å°é¢": ["é€‚åº¦æ°´è§£", "é˜²æ•", "ç§‘æ™®"]
    },
    "forbidden_words": {
        "ç¦æ­¢è¯": ["æ•å®", "å¥¶ç“¶", "å¥¶å˜´", "æ–°ç”Ÿå„¿", "è¿‡æ•", "ç–¾ç—…"],
        "ç¦ç–—æ•ˆè¡¨è¿°": ["é¢„é˜²", "ç”Ÿé•¿", "å‘è‚²", "å…ç–«"],
        "ç¦ç»å¯¹åŒ–": ["æœ€", "ç¬¬ä¸€", "TOP1", "top1", "No.1", "no.1"]
    },
    "selling_points_exact": {
        "é˜²æ•æ°´è§£æŠ€æœ¯": [
            "å¤šé¡¹ç§‘å­¦å®è¯çš„é›€å·¢å°–å³°æ°´è§£æŠ€æœ¯",
            "æ¸©å’Œçš„é€‚åº¦æ°´è§£å°åˆ†å­ç‰›å¥¶è›‹ç™½",
            "é˜²æ•é¢†åŸŸæƒå¨å¾·å›½GINIç ”ç©¶è®¤è¯",
            "èƒ½é•¿æ•ˆé˜²æ•20å¹´",
            "ç›¸æ¯”äºç‰›å¥¶è›‹ç™½è‡´æ•æ€§é™ä½1000å€"
        ],
        "è‡ªæŠ¤åŠ›": [
            "å…¨çƒåˆ›æ–°çš„è¶…å€è‡ªæŠ¤ç§‘æŠ€",
            "6ç§HMOåŠ ä¸Šæ˜æ˜ŸåŒèŒB.Infantiså’ŒBb-12",
            "ååŒä½œç”¨é‡Šæ”¾é«˜å€çš„åŸç”Ÿä¿æŠ¤åŠ›",
            "çŸ­çŸ­28å¤©å°±èƒ½è°ƒç†å¥½å¨ƒçš„è‚šè‚šèŒèŒç¯å¢ƒ",
            "ä¿æŠ¤åŠ›èƒ½æŒç»­15ä¸ªæœˆ"
        ],
        "åŸºç¡€è¥å…»": [
            "25ç§ç»´ç”Ÿç´ å’ŒçŸ¿ç‰©è´¨",
            "å…¨ä¹³ç³–çš„é…æ–¹å£å‘³æ¸…æ·¡"
        ]
    },
    "structure_requirements": {
        "æ ‡é¢˜æ•°é‡": 3,
        "æ­£æ–‡å­—æ•°ä¸Šé™": 900,
        "è¯é¢˜æ ‡ç­¾æ•°é‡": 10,
        "å¿…ætag": [
            "#èƒ½æ©å…¨æŠ¤", "#èƒ½æ©å…¨æŠ¤æ°´å¥¶", "#é€‚åº¦æ°´è§£", 
            "#é€‚åº¦æ°´è§£å¥¶ç²‰", "#é€‚åº¦æ°´è§£å¥¶ç²‰æ¨è", "#é˜²æ•å¥¶ç²‰", 
            "#ç¬¬ä¸€å£å¥¶ç²‰", "#é›€å·¢é€‚åº¦æ°´è§£"
        ]
    },
    "scoring_weights": {
        "å…³é”®è¯æ£€æŸ¥": 0.15,
        "ç¦è¯æ£€æŸ¥": 0.20,
        "å–ç‚¹è¦†ç›–": 0.30,
        "ç»“æ„å®Œæ•´æ€§": 0.20,
        "å£å»é£æ ¼": 0.15
    }
}

# ç¦è¯æ›¿æ¢å»ºè®®
FORBIDDEN_SUGGESTIONS = {
    "æ•å®": "æ•æ„Ÿä½“è´¨å®å®",
    "å¥¶ç“¶": "å–‚å…»å·¥å…·",
    "å¥¶å˜´": "å–‚å…»é…ä»¶",
    "æ–°ç”Ÿå„¿": "åˆç”Ÿå®å®",
    "è¿‡æ•": "æ•æ„Ÿ/æ•æ•",
    "ç–¾ç—…": "ä¸é€‚",
    "é¢„é˜²": "è¿œç¦»/å‡å°‘",
    "ç”Ÿé•¿": "æˆé•¿",
    "å‘è‚²": "æˆé•¿",
    "å…ç–«": "ä¿æŠ¤åŠ›/è‡ªæŠ¤åŠ›",
    "æœ€": "éå¸¸/ç‰¹åˆ«",
    "ç¬¬ä¸€": "é¢†å…ˆ/ä¼˜ç§€",
}


class ContentParser:
    """å†…å®¹è§£æå™¨"""
    
    def __init__(self, content: str):
        self.raw_content = content
        self.titles: List[str] = []
        self.body_paragraphs: List[str] = []
        self.tags: List[str] = []
        self._parse()
    
    def _parse(self):
        lines = self.raw_content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹æ ‡é¢˜
            if len(self.titles) < 3 and len(line) < 50 and not line.startswith('#'):
                if len(self.body_paragraphs) == 0:
                    self.titles.append(line)
                    continue
            
            # æ£€æµ‹è¯é¢˜æ ‡ç­¾
            tags_in_line = re.findall(r'#[\w\u4e00-\u9fff]+', line)
            if tags_in_line:
                self.tags.extend(tags_in_line)
                remaining = re.sub(r'#[\w\u4e00-\u9fff]+', '', line).strip()
                if remaining:
                    self.body_paragraphs.append(remaining)
            else:
                self.body_paragraphs.append(line)
    
    @property
    def full_text(self) -> str:
        return self.raw_content
    
    @property
    def body_text(self) -> str:
        return '\n'.join(self.body_paragraphs)
    
    @property
    def title_text(self) -> str:
        return ' '.join(self.titles)
    
    @property
    def word_count(self) -> int:
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', self.body_text)
        return len(chinese_chars)


def review_content(content: str, kol_name: str, version: str, reviewer: str) -> ReviewResult:
    """æ‰§è¡Œå®¡æ ¸"""
    result = ReviewResult(
        project_name=REVIEW_RULES["project_info"]["name"],
        kol_name=kol_name,
        version=version,
        reviewer=reviewer
    )
    
    parser = ContentParser(content)
    rules = REVIEW_RULES
    
    # 1. æ£€æŸ¥å¿…é¡»å…³é”®è¯
    required = rules.get('required_keywords', {})
    for keyword in required.get('æ ‡é¢˜', []):
        if keyword not in parser.title_text:
            result.must_fix_issues.append(ReviewIssue(
                category="å…³é”®è¯ç¼ºå¤±",
                severity=Severity.MUST_FIX,
                location="æ ‡é¢˜",
                original_text=parser.title_text[:50] if parser.title_text else "ï¼ˆç©ºï¼‰",
                problem=f"æ ‡é¢˜ç¼ºå°‘å¿…é¡»å…³é”®è¯ã€Œ{keyword}ã€",
                suggestion=f"è¯·åœ¨æ ‡é¢˜ä¸­åŠ å…¥ã€Œ{keyword}ã€"
            ))
    
    for keyword in required.get('æ­£æ–‡', []):
        if keyword not in parser.body_text:
            result.must_fix_issues.append(ReviewIssue(
                category="å…³é”®è¯ç¼ºå¤±",
                severity=Severity.MUST_FIX,
                location="æ­£æ–‡",
                original_text="",
                problem=f"æ­£æ–‡ç¼ºå°‘å¿…é¡»å…³é”®è¯ã€Œ{keyword}ã€",
                suggestion=f"è¯·åœ¨æ­£æ–‡ä¸­åŠ å…¥ã€Œ{keyword}ã€"
            ))
    
    # 2. æ£€æŸ¥ç¦è¯
    forbidden = rules.get('forbidden_words', {})
    for category, words in forbidden.items():
        for word in words:
            if word in parser.full_text:
                # è·å–ä¸Šä¸‹æ–‡
                idx = parser.full_text.find(word)
                start = max(0, idx - 10)
                end = min(len(parser.full_text), idx + len(word) + 10)
                context = parser.full_text[start:end]
                
                suggestion = FORBIDDEN_SUGGESTIONS.get(word, "è¯·åˆ é™¤æˆ–æ”¹ç”¨å…¶ä»–è¡¨è¾¾")
                if word in FORBIDDEN_SUGGESTIONS:
                    suggestion = f"å»ºè®®æ”¹ä¸ºã€Œ{FORBIDDEN_SUGGESTIONS[word]}ã€"
                
                result.must_fix_issues.append(ReviewIssue(
                    category="ç¦è¯è¿è§„",
                    severity=Severity.MUST_FIX,
                    location="æ­£æ–‡",
                    original_text=f"...{context}...",
                    problem=f"å‡ºç°{category}ã€Œ{word}ã€",
                    suggestion=suggestion
                ))
    
    # 3. æ£€æŸ¥ç²¾ç¡®å–ç‚¹
    exact_points = rules.get('selling_points_exact', {})
    found_count = 0
    total_count = 0
    
    for category, points in exact_points.items():
        for point in points:
            total_count += 1
            if point in parser.full_text:
                found_count += 1
            else:
                result.must_fix_issues.append(ReviewIssue(
                    category="å–ç‚¹ç¼ºå¤±",
                    severity=Severity.MUST_FIX,
                    location="æ­£æ–‡",
                    original_text="",
                    problem=f"ç¼ºå°‘å¿…é¡»å–ç‚¹ï¼ˆ{category}ï¼‰",
                    suggestion=f"è¯·åŠ å…¥åŸæ–‡ï¼šã€Œ{point}ã€"
                ))
    
    result.scores['å–ç‚¹è¦†ç›–'] = found_count / total_count if total_count > 0 else 1.0
    
    # 4. æ£€æŸ¥ç»“æ„
    struct_req = rules.get('structure_requirements', {})
    
    # æ ‡é¢˜æ•°é‡
    required_titles = struct_req.get('æ ‡é¢˜æ•°é‡', 3)
    if len(parser.titles) < required_titles:
        result.must_fix_issues.append(ReviewIssue(
            category="ç»“æ„é—®é¢˜",
            severity=Severity.MUST_FIX,
            location="æ ‡é¢˜",
            original_text=f"å½“å‰ï¼š{len(parser.titles)}ä¸ª",
            problem=f"æ ‡é¢˜æ•°é‡ä¸è¶³ï¼ˆè¦æ±‚{required_titles}ä¸ªï¼‰",
            suggestion=f"è¯·è¡¥å……æ ‡é¢˜ï¼Œå…±éœ€{required_titles}ä¸ª"
        ))
    
    # å­—æ•°
    max_words = struct_req.get('æ­£æ–‡å­—æ•°ä¸Šé™', 900)
    if parser.word_count > max_words:
        result.must_fix_issues.append(ReviewIssue(
            category="ç»“æ„é—®é¢˜",
            severity=Severity.MUST_FIX,
            location="æ­£æ–‡",
            original_text=f"å½“å‰ï¼š{parser.word_count}å­—",
            problem=f"å­—æ•°è¶…é™ï¼ˆä¸Šé™{max_words}å­—ï¼‰",
            suggestion=f"è¯·ç²¾ç®€å†…å®¹ï¼Œåˆ å‡{parser.word_count - max_words}å­—"
        ))
    
    # æ ‡ç­¾æ•°é‡
    required_tags = struct_req.get('è¯é¢˜æ ‡ç­¾æ•°é‡', 10)
    if len(parser.tags) < required_tags:
        result.suggest_issues.append(ReviewIssue(
            category="ç»“æ„é—®é¢˜",
            severity=Severity.SUGGEST,
            location="è¯é¢˜æ ‡ç­¾",
            original_text=f"å½“å‰ï¼š{len(parser.tags)}ä¸ª",
            problem=f"æ ‡ç­¾æ•°é‡ä¸è¶³ï¼ˆè¦æ±‚{required_tags}ä¸ªï¼‰",
            suggestion=f"è¯·è¡¥å……{required_tags - len(parser.tags)}ä¸ªè¯é¢˜æ ‡ç­¾"
        ))
    
    # å¿…ææ ‡ç­¾
    required_tags_list = struct_req.get('å¿…ætag', [])
    missing_tags = [tag for tag in required_tags_list if tag not in parser.tags]
    if missing_tags:
        result.must_fix_issues.append(ReviewIssue(
            category="ç»“æ„é—®é¢˜",
            severity=Severity.MUST_FIX,
            location="è¯é¢˜æ ‡ç­¾",
            original_text=f"ç¼ºå°‘ï¼š{', '.join(missing_tags[:3])}{'...' if len(missing_tags) > 3 else ''}",
            problem=f"ç¼ºå°‘{len(missing_tags)}ä¸ªå¿…ææ ‡ç­¾",
            suggestion=f"è¯·åŠ å…¥ï¼š{', '.join(missing_tags)}"
        ))
    
    # 5. æ£€æŸ¥å£å»
    professional_keywords = ['è¥å…»å¸ˆ', 'è‚²å©´å¸ˆ', 'åšå£«', 'ç¡•å£«', 'ä¸“ä¸š']
    has_professional = any(kw in parser.full_text for kw in professional_keywords)
    
    if not has_professional:
        result.suggest_issues.append(ReviewIssue(
            category="å£å»é—®é¢˜",
            severity=Severity.SUGGEST,
            location="å…¨æ–‡",
            original_text="",
            problem="æœªæ˜ç¡®ä½“ç°ä¸“ä¸šäººå£«èº«ä»½",
            suggestion="å»ºè®®åœ¨å¼€å¤´æ˜ç¡®èº«ä»½ï¼Œå¦‚ã€Œä½œä¸ºæŒè¯è¥å…»å¸ˆã€ã€Œè‚²å©´å¸ˆå»ºè®®ã€ç­‰"
        ))
    
    # è®¡ç®—å¾—åˆ†
    keyword_issues = len([i for i in result.must_fix_issues if 'å…³é”®è¯' in i.category])
    forbidden_issues = len([i for i in result.must_fix_issues if 'ç¦è¯' in i.category])
    structure_issues = len([i for i in result.must_fix_issues if 'ç»“æ„' in i.category])
    
    result.scores['å…³é”®è¯æ£€æŸ¥'] = max(0, 1 - keyword_issues * 0.2)
    result.scores['ç¦è¯æ£€æŸ¥'] = max(0, 1 - forbidden_issues * 0.2)
    result.scores['ç»“æ„å®Œæ•´æ€§'] = max(0, 1 - structure_issues * 0.25)
    result.scores['å£å»é£æ ¼'] = 1.0 if has_professional else 0.7
    
    # è®¡ç®—æ€»åˆ†
    weights = rules.get('scoring_weights', {})
    total = 0
    for key, weight in weights.items():
        score = result.scores.get(key, 0.5)
        total += score * weight
    result.total_score = round(total * 100, 1)
    
    # è¯†åˆ«åšå¾—å¥½çš„åœ°æ–¹
    if has_professional:
        result.good_points.append("ä¸“ä¸šèº«ä»½æ˜ç¡®")
    if result.scores.get('å–ç‚¹è¦†ç›–', 0) > 0.5:
        result.good_points.append("æ ¸å¿ƒå–ç‚¹æœ‰è¦†ç›–")
    if parser.word_count <= max_words:
        result.good_points.append(f"å­—æ•°æ§åˆ¶åˆç†ï¼ˆ{parser.word_count}å­—ï¼‰")
    
    return result


# ============================================
# Streamlit ç½‘é¡µç•Œé¢
# ============================================

st.set_page_config(
    page_title="å°çº¢ä¹¦KOLå®¡ç¨¿ç³»ç»Ÿ",
    page_icon="ğŸ”",
    layout="wide"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        padding: 1rem;
        background: linear-gradient(90deg, #ff6b6b, #ff8e53);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .score-card {
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
    }
    .score-high { background-color: #d4edda; }
    .score-medium { background-color: #fff3cd; }
    .score-low { background-color: #f8d7da; }
    .issue-card {
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 8px;
        border-left: 4px solid;
    }
    .issue-must-fix {
        background-color: #fff5f5;
        border-color: #e53e3e;
    }
    .issue-suggest {
        background-color: #fffaf0;
        border-color: #dd6b20;
    }
</style>
""", unsafe_allow_html=True)

# æ ‡é¢˜
st.markdown('<p class="main-header">ğŸ” å°çº¢ä¹¦KOLå®¡ç¨¿ç³»ç»Ÿ</p>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; color: gray;">èƒ½æ©å…¨æŠ¤ Â· å°çº¢ä¹¦è¾¾äººç§è‰é¡¹ç›®</p>', unsafe_allow_html=True)

st.markdown("---")

# è¾“å…¥åŒºåŸŸ
col1, col2, col3 = st.columns(3)

with col1:
    kol_name = st.text_input("ğŸ‘¤ KOLåç§°", placeholder="ä¾‹å¦‚ï¼šå°çº¢è–¯å¦ˆå¦ˆ")

with col2:
    version = st.selectbox("ğŸ“Œ ç‰ˆæœ¬å·", ["V1", "V2", "V3", "V4", "V5", "FINAL"])

with col3:
    reviewer = st.selectbox("ğŸ‘ï¸ å®¡æ ¸æ–¹", ["èµæ„", "å®¢æˆ·"])

# ç¨¿ä»¶è¾“å…¥
st.markdown("### ğŸ“ ç¨¿ä»¶å†…å®¹")
content = st.text_area(
    "è¯·ç²˜è´´KOLç¨¿ä»¶å†…å®¹ï¼ˆåŒ…å«æ ‡é¢˜ã€æ­£æ–‡ã€è¯é¢˜æ ‡ç­¾ï¼‰",
    height=300,
    placeholder="""ç¤ºä¾‹æ ¼å¼ï¼š

é€‚åº¦æ°´è§£å¥¶ç²‰æ€ä¹ˆé€‰ï¼Ÿé˜²æ•ç§‘æ™®æ¥äº†ï¼

ä½œä¸ºæŒè¯è¥å…»å¸ˆï¼Œæˆ‘æ¥åˆ†äº«ä¸€ä¸‹...

ï¼ˆæ­£æ–‡å†…å®¹ï¼‰

#èƒ½æ©å…¨æŠ¤ #é€‚åº¦æ°´è§£ #é˜²æ•å¥¶ç²‰ ...
"""
)

# å®¡æ ¸æŒ‰é’®
if st.button("ğŸ” å¼€å§‹å®¡æ ¸", type="primary", use_container_width=True):
    if not kol_name:
        st.error("è¯·è¾“å…¥KOLåç§°")
    elif not content.strip():
        st.error("è¯·ç²˜è´´ç¨¿ä»¶å†…å®¹")
    else:
        with st.spinner("æ­£åœ¨å®¡æ ¸..."):
            result = review_content(content, kol_name, version, reviewer)
        
        st.markdown("---")
        st.markdown("## ğŸ“Š å®¡æ ¸æŠ¥å‘Š")
        
        # åŸºæœ¬ä¿¡æ¯å’Œè¯„åˆ†
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("KOL", f"@{result.kol_name}")
        with col2:
            st.metric("ç‰ˆæœ¬", result.version)
        with col3:
            st.metric("å®¡æ ¸æ–¹", result.reviewer)
        with col4:
            score = result.total_score
            if score >= 80:
                st.metric("ç»¼åˆè¯„åˆ†", f"{score}% âœ¨")
            elif score >= 60:
                st.metric("ç»¼åˆè¯„åˆ†", f"{score}% ğŸ‘")
            else:
                st.metric("ç»¼åˆè¯„åˆ†", f"{score}% âš ï¸")
        
        # åˆ†æ•°è¯¦æƒ…
        st.markdown("### ğŸ“ˆ å„é¡¹å¾—åˆ†")
        score_cols = st.columns(len(result.scores))
        for i, (key, score) in enumerate(result.scores.items()):
            with score_cols[i]:
                score_pct = round(score * 100)
                emoji = "âœ…" if score_pct >= 80 else "âš ï¸" if score_pct >= 60 else "âŒ"
                st.metric(key, f"{emoji} {score_pct}%")
        
        # å¿…æ”¹é¡¹
        st.markdown("### âŒ å¿…æ”¹é¡¹")
        if result.must_fix_issues:
            for i, issue in enumerate(result.must_fix_issues, 1):
                with st.expander(f"{i}. ã€{issue.category}ã€‘{issue.location}", expanded=True):
                    if issue.original_text:
                        st.markdown(f"**åŸæ–‡**ï¼š`{issue.original_text}`")
                    st.markdown(f"**é—®é¢˜**ï¼š{issue.problem}")
                    st.success(f"**å»ºè®®**ï¼š{issue.suggestion}")
        else:
            st.success("ğŸ‰ æ²¡æœ‰å¿…æ”¹é¡¹ï¼")
        
        # å»ºè®®é¡¹
        st.markdown("### ğŸ’¡ å»ºè®®ä¼˜åŒ–")
        if result.suggest_issues:
            for i, issue in enumerate(result.suggest_issues, 1):
                with st.expander(f"{i}. ã€{issue.category}ã€‘{issue.location}"):
                    if issue.original_text:
                        st.markdown(f"**åŸæ–‡**ï¼š`{issue.original_text}`")
                    st.markdown(f"**é—®é¢˜**ï¼š{issue.problem}")
                    st.info(f"**å»ºè®®**ï¼š{issue.suggestion}")
        else:
            st.info("æš‚æ— ä¼˜åŒ–å»ºè®®")
        
        # åšå¾—å¥½çš„åœ°æ–¹
        st.markdown("### âœ… åšå¾—å¥½çš„åœ°æ–¹")
        if result.good_points:
            for point in result.good_points:
                st.markdown(f"- {point}")
        else:
            st.markdown("- ç»§ç»­åŠ æ²¹ï¼")
        
        # æ€»ç»“
        st.markdown("### ğŸ“ å®¡æ ¸æ€»ç»“")
        if result.total_score >= 90:
            st.success("âœ¨ **ä¼˜ç§€**ï¼šç¨¿ä»¶è´¨é‡å¾ˆé«˜ï¼Œç¨ä½œè°ƒæ•´å³å¯é€šè¿‡ï¼")
        elif result.total_score >= 75:
            st.info("ğŸ‘ **è‰¯å¥½**ï¼šæ•´ä½“ä¸é”™ï¼Œè¯·æ ¹æ®å¿…æ”¹é¡¹è¿›è¡Œä¿®æ”¹ã€‚")
        elif result.total_score >= 60:
            st.warning("âš ï¸ **éœ€æ”¹è¿›**ï¼šå­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œè¯·ä»”ç»†ä¿®æ”¹åé‡æ–°æäº¤ã€‚")
        else:
            st.error("âŒ **éœ€å¤§æ”¹**ï¼šé—®é¢˜è¾ƒå¤šï¼Œå»ºè®®å‚è€ƒbriefé‡æ–°æ’°å†™ã€‚")
        
        # ä¸‹è½½æŠ¥å‘Š
        report_text = f"""# å®¡æ ¸æŠ¥å‘Š

## åŸºç¡€ä¿¡æ¯
- é¡¹ç›®ï¼š{result.project_name}
- KOLï¼š@{result.kol_name}
- ç‰ˆæœ¬ï¼š{result.version}
- å®¡æ ¸æ–¹ï¼š{result.reviewer}
- ç»¼åˆè¯„åˆ†ï¼š{result.total_score}%
- å®¡æ ¸æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}

## å¿…æ”¹é¡¹ï¼ˆ{len(result.must_fix_issues)}æ¡ï¼‰
"""
        for i, issue in enumerate(result.must_fix_issues, 1):
            report_text += f"\n{i}. ã€{issue.category}ã€‘{issue.location}\n"
            if issue.original_text:
                report_text += f"   åŸæ–‡ï¼š{issue.original_text}\n"
            report_text += f"   é—®é¢˜ï¼š{issue.problem}\n"
            report_text += f"   å»ºè®®ï¼š{issue.suggestion}\n"
        
        report_text += f"\n## å»ºè®®ä¼˜åŒ–ï¼ˆ{len(result.suggest_issues)}æ¡ï¼‰\n"
        for i, issue in enumerate(result.suggest_issues, 1):
            report_text += f"\n{i}. ã€{issue.category}ã€‘{issue.location}\n"
            report_text += f"   é—®é¢˜ï¼š{issue.problem}\n"
            report_text += f"   å»ºè®®ï¼š{issue.suggestion}\n"
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å®¡æ ¸æŠ¥å‘Š",
            data=report_text,
            file_name=f"å®¡æ ¸æŠ¥å‘Š_{kol_name}_{version}.md",
            mime="text/markdown"
        )

# é¡µè„š
st.markdown("---")
st.markdown(
    '<p style="text-align: center; color: gray; font-size: 0.8rem;">'
    'å°çº¢ä¹¦KOLå®¡ç¨¿ç³»ç»Ÿ v1.0 | èƒ½æ©å…¨æŠ¤é¡¹ç›®ä¸“ç”¨'
    '</p>', 
    unsafe_allow_html=True
)
